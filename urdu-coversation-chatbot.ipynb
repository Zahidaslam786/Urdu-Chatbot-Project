{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6329548,"sourceType":"datasetVersion","datasetId":3640718}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q gradio nltk rouge-score sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:34:38.929266Z","iopub.execute_input":"2025-10-20T07:34:38.930171Z","iopub.status.idle":"2025-10-20T07:34:42.318202Z","shell.execute_reply.started":"2025-10-20T07:34:38.930136Z","shell.execute_reply":"2025-10-20T07:34:42.317408Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nimport pandas as pd\nimport numpy as np\nimport unicodedata\nimport re\nimport random\nimport math\nimport os\nimport time\nfrom tqdm import tqdm\n\nimport sentencepiece as spm\n\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.chrf_score import corpus_chrf\nnltk.download('punkt') \n\nimport gradio as gr\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# Check device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\" Libraries imported. Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:34:53.802023Z","iopub.execute_input":"2025-10-20T07:34:53.802771Z","iopub.status.idle":"2025-10-20T07:35:07.645520Z","shell.execute_reply.started":"2025-10-20T07:34:53.802744Z","shell.execute_reply":"2025-10-20T07:35:07.644708Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":" Libraries imported. Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Load & Inspect Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nDATASET_PATH = \"/kaggle/input/urdu-dataset-20000/final_main_dataset.tsv\"\n\ntry:\n    df_raw = pd.read_csv(DATASET_PATH, sep='\\t', on_bad_lines='skip')\n    \n    if 'sentence' not in df_raw.columns:\n        print(\"Error: 'sentence' column not found!\")\n    else:\n        df = df_raw[['sentence']].copy()\n        \n        df = df.dropna(subset=['sentence'])\n        df = df.drop_duplicates(subset=['sentence'])\n        \n        print(f\"Data shape after cleaning (rows, cols): {df.shape}\")\n        print(\"--- Random Sample (Raw) ---\")\n        print(df.sample(3))\n\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:14.865980Z","iopub.execute_input":"2025-10-20T07:35:14.866510Z","iopub.status.idle":"2025-10-20T07:35:15.112789Z","shell.execute_reply.started":"2025-10-20T07:35:14.866488Z","shell.execute_reply":"2025-10-20T07:35:15.111997Z"}},"outputs":[{"name":"stdout","text":"Data shape after cleaning (rows, cols): (10699, 1)\n--- Random Sample (Raw) ---\n                                               sentence\n3704       بچوں پر تعلیم کے حوالے سے مضراثرات مرتب ہوئے\n6427           اب میں مزیداِس بات کو برداشت نہیں کرسکتا\n304   جب کوئی اختلافی صورت نمودار ہوتی، خلیفہ کے سام...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Preprocessing (Cleaning Function)","metadata":{}},{"cell_type":"code","source":"import unicodedata\nimport re\n\ndef normalize_urdu_text(text: str) -> str:\n    \"\"\"\n    Normalize Urdu text:\n    1. Remove diacritics (zer, zabar, pesh).\n    2. Standardize Alef and Yeh forms.\n    3. Keep only Urdu characters, digits, and basic punctuation.\n    4. Collapse multiple spaces.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    text = ''.join(c for c in unicodedata.normalize('NFD', text) \n                   if unicodedata.category(c) != 'Mn')\n    \n    text = re.sub(r'[آأإ]', 'ا', text) # Standardize Alef\n    text = re.sub(r'[ى]', 'ی', text)  # Standardize Yeh\n    \n    text = re.sub(r'[^\\u0600-\\u06FF\\s\\d\\.!؟،]', '', text)\n    \n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ntest_sentence = \"یہ ایک ٹیسٹ ہے، اِس میں 123 نمبرز بھی ہیں!\"\nprint(f\"Original:  {test_sentence}\")\nprint(f\"Cleaned:   {normalize_urdu_text(test_sentence)}\")\nprint(\"\\n Normalization function created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:19.014769Z","iopub.execute_input":"2025-10-20T07:35:19.015504Z","iopub.status.idle":"2025-10-20T07:35:19.022268Z","shell.execute_reply.started":"2025-10-20T07:35:19.015479Z","shell.execute_reply":"2025-10-20T07:35:19.021554Z"}},"outputs":[{"name":"stdout","text":"Original:  یہ ایک ٹیسٹ ہے، اِس میں 123 نمبرز بھی ہیں!\nCleaned:   یہ ایک ٹیسٹ ہے، اس میں 123 نمبرز بھی ہیں!\n\n Normalization function created.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Apply Cleaning & Create (Input, Response) Pairs","metadata":{}},{"cell_type":"code","source":"print(f\"Applying normalization to {len(df)} sentences...\")\ndf['cleaned_sentence'] = df['sentence'].apply(normalize_urdu_text)\n\ninputs = []\nresponses = []\n\nfor text in tqdm(df['cleaned_sentence'], desc=\"Creating Pairs\"):\n    words = text.split()\n    if len(words) >= 2:\n        mid_point = len(words) // 2\n        \n        # Handle 1-word-each case\n        if mid_point == 0: mid_point = 1 \n            \n        input_text = ' '.join(words[:mid_point])\n        response_text = ' '.join(words[mid_point:])\n        \n        # Ensure neither side is empty\n        if input_text and response_text:\n            inputs.append(input_text)\n            responses.append(response_text)\n\ndata_pairs = pd.DataFrame({\n    'input': inputs,\n    'response': responses\n})\ndata_pairs = data_pairs.dropna()\n\nprint(f\"\\nTotal conversational pairs created: {len(data_pairs)}\")\nprint(\"--- Sample Pairs (Split Logic) ---\")\nprint(data_pairs.sample(3))\ndf = data_pairs ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:44.407589Z","iopub.execute_input":"2025-10-20T07:35:44.408146Z","iopub.status.idle":"2025-10-20T07:35:44.579162Z","shell.execute_reply.started":"2025-10-20T07:35:44.408125Z","shell.execute_reply":"2025-10-20T07:35:44.578519Z"}},"outputs":[{"name":"stdout","text":"Applying normalization to 10699 sentences...\n","output_type":"stream"},{"name":"stderr","text":"Creating Pairs: 100%|██████████| 10699/10699 [00:00<00:00, 491827.78it/s]","output_type":"stream"},{"name":"stdout","text":"\nTotal conversational pairs created: 10519\n--- Sample Pairs (Split Logic) ---\n                  input            response\n1056           ٹیسٹ میں   بیکٹیریا نہیں تھا\n2982  اج موقف ایک ہی ہے  جو نواز شریف کا ہے\n4795     عمرا ن خان میں  بہت سی خوبیاں ہوں۔\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Split Data (Train, Val, Test)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We will use the 'input' and 'response' columns\ndata_pairs = df[['input', 'response']]\n\ntrain_df, temp_df = train_test_split(\n    data_pairs,\n    test_size=0.2, # 20% for validation and test\n    random_state=SEED # Ensures reproducible split\n)\n\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=0.5, # 0.5 * 0.2 = 0.1 (10% of total)\n    random_state=SEED # Ensures reproducible split\n)\n\n# Reset index just for clean looks (optional)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\nprint(f\"Data splitting complete:\")\nprint(f\"  Train set:    {len(train_df)} pairs\")\nprint(f\"  Validation set: {len(val_df)} pairs\")\nprint(f\"  Test set:     {len(test_df)} pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:51.116940Z","iopub.execute_input":"2025-10-20T07:35:51.117174Z","iopub.status.idle":"2025-10-20T07:35:51.127580Z","shell.execute_reply.started":"2025-10-20T07:35:51.117160Z","shell.execute_reply":"2025-10-20T07:35:51.126819Z"}},"outputs":[{"name":"stdout","text":"Data splitting complete:\n  Train set:    8415 pairs\n  Validation set: 1052 pairs\n  Test set:     1052 pairs\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Create Corpus for Tokenizer","metadata":{}},{"cell_type":"code","source":"\nprint(\"Creating corpus file for tokenizer...\")\n\n# Define the corpus file path\ncorpus_path = 'urdu_corpus.txt'\n\n# Open the file in write mode (encoding='utf-8')\nwith open(corpus_path, 'w', encoding='utf-8') as f:\n    # Add 'input' sentences\n    for text in train_df['input']:\n        f.write(text + '\\n')\n    # Add 'response' sentences\n    for text in train_df['response']:\n        f.write(text + '\\n')\n\nprint(f\" Corpus file created at: {corpus_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:53.783832Z","iopub.execute_input":"2025-10-20T07:35:53.784284Z","iopub.status.idle":"2025-10-20T07:35:53.799691Z","shell.execute_reply.started":"2025-10-20T07:35:53.784261Z","shell.execute_reply":"2025-10-20T07:35:53.798918Z"}},"outputs":[{"name":"stdout","text":"Creating corpus file for tokenizer...\n Corpus file created at: urdu_corpus.txt\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### Train SentencePiece Tokenizer","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\n\nprint(\"Training SentencePiece tokenizer...\")\n\n# Define paths and parameters\ncorpus = 'urdu_corpus.txt'\nmodel_prefix = 'urdu_spm' # Will create urdu_spm.model and urdu_spm.vocab\n\n\nvocab_size = 6300 # Set to a value \n\n# Define special token IDs\nPAD_ID = 0\nUNK_ID = 1\nSOS_ID = 2 # Start of Sentence\nEOS_ID = 3 # End of Sentence\n\n# Train the model\nspm.SentencePieceTrainer.Train(\n    input=corpus,\n    model_prefix=model_prefix,\n    vocab_size=vocab_size,\n    model_type='unigram',\n    character_coverage=1.0,\n    pad_id=PAD_ID,\n    unk_id=UNK_ID,\n    bos_id=SOS_ID,\n    eos_id=EOS_ID,\n    pad_piece='<pad>',\n    unk_piece='<unk>',\n    bos_piece='<sos>',\n    eos_piece='<eos>'\n)\n\nprint(f\" Tokenizer training complete. Vocab size: {vocab_size}\")\nprint(f\"Files created: {model_prefix}.model, {model_prefix}.vocab\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:35:56.522113Z","iopub.execute_input":"2025-10-20T07:35:56.522589Z","iopub.status.idle":"2025-10-20T07:35:56.815779Z","shell.execute_reply.started":"2025-10-20T07:35:56.522567Z","shell.execute_reply":"2025-10-20T07:35:56.814836Z"}},"outputs":[{"name":"stdout","text":"Training SentencePiece tokenizer...\n Tokenizer training complete. Vocab size: 6300\nFiles created: urdu_spm.model, urdu_spm.vocab\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: urdu_corpus.txt\n  input_format: \n  model_prefix: urdu_spm\n  model_type: UNIGRAM\n  vocab_size: 6300\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 1\n  bos_id: 2\n  eos_id: 3\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <sos>\n  eos_piece: <eos>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: urdu_corpus.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 16830 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <sos>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <eos>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=323067\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=60\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 16830 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=165388\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 15907 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16830\ntrainer_interface.cc(609) LOG(INFO) Done! 9779\nunigram_model_trainer.cc(602) LOG(INFO) Using 9779 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7561 obj=9.64993 num_tokens=16785 num_tokens/piece=2.21994\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6603 obj=8.51052 num_tokens=16833 num_tokens/piece=2.5493\ntrainer_interface.cc(687) LOG(INFO) Saving model: urdu_spm.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: urdu_spm.vocab\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Load and Test Tokenizer","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\n\n# Load the trained model\nsp = spm.SentencePieceProcessor(model_file='urdu_spm.model')\n\n# --- Test ---\nprint(f\"Vocabulary size: {sp.vocab_size()}\")\n\n# Check special token IDs\nprint(f\"PAD ID: {sp.pad_id()}\")\nprint(f\"SOS ID: {sp.bos_id()}\") # bos = <sos>\nprint(f\"EOS ID: {sp.eos_id()}\") # eos = <eos>\nprint(f\"UNK ID: {sp.unk_id()}\")\n\n# Test encoding and decoding\ntest_sentence = \"یہ ایک ٹیسٹ ہے\"\nprint(f\"\\nOriginal: '{test_sentence}'\")\n\nencoded_pieces = sp.encode(test_sentence, out_type=str)\nprint(f\"Encoded (pieces): {encoded_pieces}\")\n\nencoded_ids = sp.encode(test_sentence, out_type=int)\nprint(f\"Encoded (IDs): {encoded_ids}\")\n\ndecoded_text = sp.decode(encoded_ids)\nprint(f\"Decoded: '{decoded_text}'\")\n\nprint(f\"\\n Tokenizer loaded and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:36:01.467356Z","iopub.execute_input":"2025-10-20T07:36:01.467717Z","iopub.status.idle":"2025-10-20T07:36:01.485268Z","shell.execute_reply.started":"2025-10-20T07:36:01.467696Z","shell.execute_reply":"2025-10-20T07:36:01.484462Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 6300\nPAD ID: 0\nSOS ID: 2\nEOS ID: 3\nUNK ID: 1\n\nOriginal: 'یہ ایک ٹیسٹ ہے'\nEncoded (pieces): ['▁یہ', '▁ایک', '▁ٹیسٹ', '▁ہے']\nEncoded (IDs): [18, 28, 284, 8]\nDecoded: 'یہ ایک ٹیسٹ ہے'\n\n Tokenizer loaded and tested.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Create PyTorch Dataset Class","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport random\n\n# --- Constants ---\nMAX_LEN = 60\nVOCAB_SIZE = sp.vocab_size()\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nPAD_ID, UNK_ID, SOS_ID, EOS_ID = sp.pad_id(), sp.unk_id(), sp.bos_id(), sp.eos_id()\nMASK_TOKEN = UNK_ID # Use <unk> (ID 1) as mask token\n\ndef corrupt_tokens(token_ids, mask_prob=0.15, max_span_len=3):\n    corrupted_ids = list(token_ids)\n    n_tokens = len(corrupted_ids)\n    num_to_mask = int(n_tokens * mask_prob)\n    masked_indices = set()\n    if n_tokens < 2: return corrupted_ids # Cannot corrupt\n    \n    while len(masked_indices) < num_to_mask:\n        span_len = random.randint(1, max_span_len)\n        start_idx = random.randint(0, n_tokens - span_len)\n        for i in range(span_len):\n            idx = start_idx + i\n            if idx < n_tokens and idx not in masked_indices:\n                corrupted_ids[idx] = MASK_TOKEN\n                masked_indices.add(idx)\n    return corrupted_ids\n\nclass UrduChatDataset(Dataset):\n    def __init__(self, dataframe, sp_tokenizer, max_len=MAX_LEN):\n        self.dataframe = dataframe\n        self.sp = sp_tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def tokenize_and_pad(self, text, max_len):\n        token_ids = self.sp.encode(text, out_type=int)\n        token_ids = token_ids[:max_len - 2] # Truncate\n        final_ids = [SOS_ID] + token_ids + [EOS_ID]\n        pad_len = max_len - len(final_ids)\n        return torch.tensor(final_ids + ([PAD_ID] * pad_len), dtype=torch.long)\n\n    def __getitem__(self, idx):\n        input_text = self.dataframe.loc[idx, 'input']\n        response_text = self.dataframe.loc[idx, 'response']\n        \n        tgt_tensor = self.tokenize_and_pad(response_text, self.max_len)\n        \n        src_token_ids = self.sp.encode(input_text, out_type=int)\n        src_token_ids = src_token_ids[:self.max_len - 2]\n        \n        corrupted_ids = corrupt_tokens(src_token_ids) # Apply corruption\n        \n        final_src_ids = [SOS_ID] + corrupted_ids + [EOS_ID]\n        pad_len = self.max_len - len(final_src_ids)\n        src_tensor = torch.tensor(final_src_ids + ([PAD_ID] * pad_len), dtype=torch.long)\n        \n        return src_tensor, tgt_tensor\n\n# --- Test the Dataset class ---\nprint(\"Testing Dataset class (Split + Corruption)...\")\ntest_data = pd.DataFrame({'input': ['کیا ہم یہ'], 'response': ['کر سکتے ہیں؟']})\ntest_dataset = UrduChatDataset(test_data, sp, max_len=20)\nsrc, tgt = test_dataset[0]\n\nprint(f\"Source Text (Input): 'کیا ہم یہ'\")\nprint(f\"Target Text (Resp): 'کر سکتے ہیں؟'\")\nprint(f\"Source Tensor (Corrupted):\\n {src}\")\nprint(f\"Target Tensor (Clean):\\n {tgt}\")\nprint(f\"\\n Dataset class (Split + Corruption) created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:36:08.357618Z","iopub.execute_input":"2025-10-20T07:36:08.358223Z","iopub.status.idle":"2025-10-20T07:36:08.435237Z","shell.execute_reply.started":"2025-10-20T07:36:08.358200Z","shell.execute_reply":"2025-10-20T07:36:08.434409Z"}},"outputs":[{"name":"stdout","text":"Testing Dataset class (Split + Corruption)...\nSource Text (Input): 'کیا ہم یہ'\nTarget Text (Resp): 'کر سکتے ہیں؟'\nSource Tensor (Corrupted):\n tensor([ 2, 29, 44, 18,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0])\nTarget Tensor (Clean):\n tensor([  2,  27, 250,  25,  36,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0])\n\n Dataset class (Split + Corruption) created and tested.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Create DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 30 # As suggested in the assignment\n\ntrain_dataset = UrduChatDataset(train_df, sp, MAX_LEN)\nval_dataset = UrduChatDataset(val_df, sp, MAX_LEN)\ntest_dataset = UrduChatDataset(test_df, sp, MAX_LEN)\n\n# We shuffle the training data to improve model generalization\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\n# No need to shuffle validation or test data\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nprint(f\" DataLoaders created successfully.\")\nprint(f\"Total training batches: {len(train_loader)}\")\nprint(f\"Total validation batches: {len(val_loader)}\")\n\n# --- Test one batch ---\nprint(\"\\nTesting one batch from train_loader...\")\nsrc_batch, tgt_batch = next(iter(train_loader))\nprint(f\"Source batch shape: {src_batch.shape}\") # [BATCH_SIZE, MAX_LEN]\nprint(f\"Target batch shape: {tgt_batch.shape}\") # [BATCH_SIZE, MAX_LEN]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:39:06.558018Z","iopub.execute_input":"2025-10-20T07:39:06.558534Z","iopub.status.idle":"2025-10-20T07:39:06.624897Z","shell.execute_reply.started":"2025-10-20T07:39:06.558512Z","shell.execute_reply":"2025-10-20T07:39:06.624154Z"}},"outputs":[{"name":"stdout","text":" DataLoaders created successfully.\nTotal training batches: 281\nTotal validation batches: 36\n\nTesting one batch from train_loader...\nSource batch shape: torch.Size([30, 60])\nTarget batch shape: torch.Size([30, 60])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Model Hyperparameters & Positional Encoding","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nEMBED_DIM = 256\nNUM_HEADS = 2\nENCODER_LAYERS = 2\nDECODER_LAYERS = 2\nFFN_DIM = 1024\nDROPOUT = 0.1 # Start with 0.1\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=MAX_LEN):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model); position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1); div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)); pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term); pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]\n        \nprint(f\" PositionalEncoding created. Model size reset to: 256 Dim, 2 Layers.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:39:11.849516Z","iopub.execute_input":"2025-10-20T07:39:11.849908Z","iopub.status.idle":"2025-10-20T07:39:11.857592Z","shell.execute_reply.started":"2025-10-20T07:39:11.849885Z","shell.execute_reply":"2025-10-20T07:39:11.856488Z"}},"outputs":[{"name":"stdout","text":" PositionalEncoding created. Model size reset to: 256 Dim, 2 Layers.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Multi-Head Attention Class","metadata":{}},{"cell_type":"code","source":"# Cell 13: Multi-Head Attention Class\nimport torch.nn.functional as F\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Implements the Multi-Head Attention mechanism from scratch.\n    \"\"\"\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        # d_model must be divisible by num_heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model       # Total embedding dimension (e.g., 256)\n        self.num_heads = num_heads   # Number of heads (e.g., 2)\n        self.d_k = d_model // num_heads # Dimension of each head (e.g., 256 // 2 = 128)\n        \n        # Linear layers for Query, Key, Value, and the final output\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q, K, V shape: [batch_size, num_heads, seq_len, d_k]\n        \n        # scores shape: [batch_size, num_heads, seq_len, seq_len]\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            # Fill with a very small number where mask is 0\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        # attn shape: [batch_size, num_heads, seq_len, seq_len]\n        attn = F.softmax(scores, dim=-1)\n        \n        # context shape: [batch_size, num_heads, seq_len, d_k]\n        context = torch.matmul(attn, V)\n        \n        return context\n\n    def forward(self, Q, K, V, mask=None):\n        # Q, K, V input shape: [batch_size, seq_len, d_model]\n        batch_size = Q.size(0)\n        \n        Q = self.W_q(Q)\n        K = self.W_k(K)\n        V = self.W_v(V)\n        \n        # [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, d_k]\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        context = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # [batch_size, num_heads, seq_len, d_k] -> [batch_size, seq_len, d_model]\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        output = self.W_o(context)\n        \n        # output shape: [batch_size, seq_len, d_model]\n        return output\n\n# --- Test ---\nprint(f\"Testing MultiHeadAttention...\")\nmha_layer = MultiHeadAttention(d_model=EMBED_DIM, num_heads=NUM_HEADS).to(DEVICE)\ntest_tensor = torch.rand(BATCH_SIZE, MAX_LEN, EMBED_DIM).to(DEVICE) # Dummy batch\n\n# In self-attention (like in Encoder), Q, K, and V are the same tensor\noutput_tensor = mha_layer(test_tensor, test_tensor, test_tensor, mask=None)\n\nprint(f\"Input shape:  {test_tensor.shape}\")\nprint(f\"Output shape: {output_tensor.shape}\")\nprint(f\" MultiHeadAttention class created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:39:15.523608Z","iopub.execute_input":"2025-10-20T07:39:15.524211Z","iopub.status.idle":"2025-10-20T07:39:16.126892Z","shell.execute_reply.started":"2025-10-20T07:39:15.524190Z","shell.execute_reply":"2025-10-20T07:39:16.126236Z"}},"outputs":[{"name":"stdout","text":"Testing MultiHeadAttention...\nInput shape:  torch.Size([30, 60, 256])\nOutput shape: torch.Size([30, 60, 256])\n MultiHeadAttention class created and tested.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Feed-Forward Network Class","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n\n    def __init__(self, d_model, d_ff, dropout=DROPOUT):\n        super(FeedForward, self).__init__()\n        \n        # FFN consists of two linear layers with a ReLU activation\n        self.linear_1 = nn.Linear(d_model, d_ff) # e.g., 256 -> 1024\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # e.g., 1024 -> 256\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, d_model]\n        \n        x = self.linear_1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.linear_2(x)\n        \n        # output shape: [batch_size, seq_len, d_model]\n        return x\n\n# --- Test ---\nprint(f\"Testing FeedForward...\")\nffn_layer = FeedForward(d_model=EMBED_DIM, d_ff=FFN_DIM).to(DEVICE)\ntest_tensor = torch.rand(BATCH_SIZE, MAX_LEN, EMBED_DIM).to(DEVICE) # Dummy batch\n\noutput_tensor = ffn_layer(test_tensor)\n\nprint(f\"Input shape:  {test_tensor.shape}\")\nprint(f\"Output shape: {output_tensor.shape}\")\nprint(f\" FeedForward class created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:40:46.753003Z","iopub.execute_input":"2025-10-20T07:40:46.753773Z","iopub.status.idle":"2025-10-20T07:40:46.873788Z","shell.execute_reply.started":"2025-10-20T07:40:46.753747Z","shell.execute_reply":"2025-10-20T07:40:46.873158Z"}},"outputs":[{"name":"stdout","text":"Testing FeedForward...\nInput shape:  torch.Size([30, 60, 256])\nOutput shape: torch.Size([30, 60, 256])\n FeedForward class created and tested.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Encoder Layer Class","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=DROPOUT):\n        super(EncoderLayer, self).__init__()\n        \n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model) # Layer normalization\n        \n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model) # Layer normalization\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        # Note: Q, K, and V are all 'x' in self-attention\n        attn_output = self.self_attn(x, x, x, mask)\n        # Add & Norm: Add the original input (residual)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        ff_output = self.feed_forward(x)\n        # Add & Norm\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        # output shape: [batch_size, seq_len, d_model]\n        return x\n\n# --- Test ---\nprint(f\"Testing EncoderLayer...\")\nencoder_layer = EncoderLayer(EMBED_DIM, NUM_HEADS, FFN_DIM).to(DEVICE)\ntest_tensor = torch.rand(BATCH_SIZE, MAX_LEN, EMBED_DIM).to(DEVICE)\n\n\ntest_mask = (torch.ones(BATCH_SIZE, MAX_LEN) > 0.5).unsqueeze(1).unsqueeze(2).to(DEVICE) # Dummy mask\n\noutput_tensor = encoder_layer(test_tensor, test_mask)\n\nprint(f\"Input shape:  {test_tensor.shape}\")\nprint(f\"Mask shape:   {test_mask.shape}\")\nprint(f\"Output shape: {output_tensor.shape}\")\nprint(f\" EncoderLayer class created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:40:52.214585Z","iopub.execute_input":"2025-10-20T07:40:52.214852Z","iopub.status.idle":"2025-10-20T07:40:52.524189Z","shell.execute_reply.started":"2025-10-20T07:40:52.214832Z","shell.execute_reply":"2025-10-20T07:40:52.523536Z"}},"outputs":[{"name":"stdout","text":"Testing EncoderLayer...\nInput shape:  torch.Size([30, 60, 256])\nMask shape:   torch.Size([30, 1, 1, 60])\nOutput shape: torch.Size([30, 60, 256])\n EncoderLayer class created and tested.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Decoder Layer Class","metadata":{}},{"cell_type":"code","source":"\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=DROPOUT):\n        super(DecoderLayer, self).__init__()\n        \n        # Sub-layer 1: Masked Multi-Head Attention\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        \n        # Sub-layer 2: Cross-Attention (with Encoder output)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Sub-layer 3: Feed-Forward Network\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm3 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n\n        # Q, K, V are all 'x' (decoder input)\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Q is from decoder ('x'), K and V are from encoder ('enc_output')\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        \n        # output shape: [batch_size, tgt_seq_len, d_model]\n        return x\n\n# --- Test ---\nprint(f\"Testing DecoderLayer...\")\ndecoder_layer = DecoderLayer(EMBED_DIM, NUM_HEADS, FFN_DIM).to(DEVICE)\n\n# Dummy tensors\ntest_tgt_tensor = torch.rand(BATCH_SIZE, MAX_LEN, EMBED_DIM).to(DEVICE) # Decoder input\ntest_enc_output = torch.rand(BATCH_SIZE, MAX_LEN, EMBED_DIM).to(DEVICE) # Encoder output\n\n# Dummy masks\ntest_src_mask = (torch.ones(BATCH_SIZE, MAX_LEN) > 0.5).unsqueeze(1).unsqueeze(2).to(DEVICE)\n# Decoder target mask (look-ahead)\ntest_tgt_sub_mask = torch.tril(torch.ones(MAX_LEN, MAX_LEN)).bool().to(DEVICE)\ntest_tgt_mask = test_src_mask & test_tgt_sub_mask # Combine padding and look-ahead\n\noutput_tensor = decoder_layer(test_tgt_tensor, test_enc_output, test_src_mask, test_tgt_mask)\n\nprint(f\"Decoder Input shape: {test_tgt_tensor.shape}\")\nprint(f\"Encoder Output shape: {test_enc_output.shape}\")\nprint(f\"Source Mask shape:   {test_src_mask.shape}\")\nprint(f\"Target Mask shape:   {test_tgt_mask.shape}\")\nprint(f\"Final Output shape:  {output_tensor.shape}\")\nprint(f\" DecoderLayer class created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:42:00.216320Z","iopub.execute_input":"2025-10-20T07:42:00.217065Z","iopub.status.idle":"2025-10-20T07:42:00.299765Z","shell.execute_reply.started":"2025-10-20T07:42:00.217042Z","shell.execute_reply":"2025-10-20T07:42:00.299102Z"}},"outputs":[{"name":"stdout","text":"Testing DecoderLayer...\nDecoder Input shape: torch.Size([30, 60, 256])\nEncoder Output shape: torch.Size([30, 60, 256])\nSource Mask shape:   torch.Size([30, 1, 1, 60])\nTarget Mask shape:   torch.Size([30, 1, 60, 60])\nFinal Output shape:  torch.Size([30, 60, 256])\n DecoderLayer class created and tested.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Transformer Model (Main Class)","metadata":{}},{"cell_type":"code","source":"\nclass Transformer(nn.Module):\n    def __init__(self, \n                 src_vocab_size, \n                 tgt_vocab_size, \n                 d_model, \n                 num_heads, \n                 num_encoder_layers, \n                 num_decoder_layers, \n                 d_ff, \n                 max_len, \n                 dropout=DROPOUT,\n                 pad_idx=PAD_ID):\n        \n        super(Transformer, self).__init__()\n        \n        # --- Embeddings ---\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_idx)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_idx)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        \n        # --- Encoder Stack ---\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout) \n            for _ in range(num_encoder_layers)\n        ])\n        \n        # --- Decoder Stack ---\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        # Projects decoder output (d_model) to vocabulary size\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.pad_idx = pad_idx\n\n    def _generate_mask(self, src, tgt):\n        # src shape: [batch_size, src_seq_len]\n        # tgt shape: [batch_size, tgt_seq_len]\n        # 1. Source Padding Mask\n        # [batch_size, 1, 1, src_seq_len]\n        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n\n        # [batch_size, 1, tgt_seq_len, 1]\n        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)\n        \n        tgt_seq_len = tgt.size(1)\n        # [1, 1, tgt_seq_len, tgt_seq_len]\n        tgt_sub_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=DEVICE)).bool()\n        \n        # [batch_size, 1, tgt_seq_len, tgt_seq_len]\n        tgt_mask = tgt_pad_mask & tgt_sub_mask\n        \n        return src_mask, tgt_mask\n\n    def encode(self, src, src_mask):\n        src_emb = self.dropout(self.pos_encoding(self.src_embedding(src)))\n        \n        enc_output = src_emb\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n            \n        return enc_output\n\n    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n        tgt_emb = self.dropout(self.pos_encoding(self.tgt_embedding(tgt)))\n        \n        dec_output = tgt_emb\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n            \n        return dec_output\n\n    def forward(self, src, tgt):\n        # src: [batch_size, src_seq_len]\n        # tgt: [batch_size, tgt_seq_len]\n        \n        src_mask, tgt_mask = self._generate_mask(src, tgt)\n        \n        # enc_output: [batch_size, src_seq_len, d_model]\n        enc_output = self.encode(src, src_mask)\n        \n        # dec_output: [batch_size, tgt_seq_len, d_model]\n        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n        \n        # output: [batch_size, tgt_seq_len, tgt_vocab_size]\n        output = self.fc_out(dec_output)\n        \n        return output\n\n# --- Test ---\nprint(f\"Testing Transformer model...\")\n# Instantiate the model\ntransformer_model = Transformer(\n    src_vocab_size=VOCAB_SIZE,\n    tgt_vocab_size=VOCAB_SIZE,\n    d_model=EMBED_DIM,\n    num_heads=NUM_HEADS,\n    num_encoder_layers=ENCODER_LAYERS,\n    num_decoder_layers=DECODER_LAYERS,\n    d_ff=FFN_DIM,\n    max_len=MAX_LEN,\n    dropout=DROPOUT,\n    pad_idx=PAD_ID\n).to(DEVICE)\n\n# Get a test batch from our DataLoader\nsrc_batch, tgt_batch = next(iter(train_loader))\nsrc_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n\n\noutput_logits = transformer_model(src_batch, tgt_batch[:, :-1])\n\nprint(f\"Source batch shape:  {src_batch.shape}\")\nprint(f\"Target batch shape (input): {tgt_batch[:, :-1].shape}\")\nprint(f\"Model output shape:  {output_logits.shape}\")\nprint(f\"  (Expected: [Batch, SeqLen-1, VocabSize])\")\nprint(f\"  (Actual:   [{output_logits.shape[0]}, {output_logits.shape[1]}, {output_logits.shape[2]}])\")\n\nprint(f\"\\n Transformer class created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:42:38.836753Z","iopub.execute_input":"2025-10-20T07:42:38.837438Z","iopub.status.idle":"2025-10-20T07:42:39.112585Z","shell.execute_reply.started":"2025-10-20T07:42:38.837415Z","shell.execute_reply":"2025-10-20T07:42:39.111765Z"}},"outputs":[{"name":"stdout","text":"Testing Transformer model...\nSource batch shape:  torch.Size([30, 60])\nTarget batch shape (input): torch.Size([30, 59])\nModel output shape:  torch.Size([30, 59, 6300])\n  (Expected: [Batch, SeqLen-1, VocabSize])\n  (Actual:   [30, 59, 6300])\n\n Transformer class created and tested.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Initialize Model, Loss, and Optimizer","metadata":{}},{"cell_type":"code","source":"model = Transformer(\n    src_vocab_size=VOCAB_SIZE, tgt_vocab_size=VOCAB_SIZE,\n    d_model=EMBED_DIM, num_heads=NUM_HEADS,\n    num_encoder_layers=ENCODER_LAYERS, num_decoder_layers=DECODER_LAYERS,\n    d_ff=FFN_DIM, max_len=MAX_LEN, dropout=DROPOUT, pad_idx=PAD_ID\n).to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\nLEARNING_RATE = 1e-4 # (Aap ke dost wala LR)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ndef count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\" LARGER Model (Dropout={DROPOUT}), Criterion, Optimizer initialized.\")\nprint(f\"   Model has {count_parameters(model):,} parameters.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:42:51.658097Z","iopub.execute_input":"2025-10-20T07:42:51.658907Z","iopub.status.idle":"2025-10-20T07:42:55.300611Z","shell.execute_reply.started":"2025-10-20T07:42:51.658876Z","shell.execute_reply":"2025-10-20T07:42:55.299896Z"}},"outputs":[{"name":"stdout","text":" LARGER Model (Dropout=0.1), Criterion, Optimizer initialized.\n   Model has 8,531,100 parameters.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Training Loop Function (One Epoch)","metadata":{}},{"cell_type":"code","source":"import time\n\ndef train_epoch(model, dataloader, optimizer, criterion, device, clip=1):\n    \"\"\"\n    Trains the model for one epoch.\n    \"\"\"\n    # Set the model to training mode (enables dropout)\n    model.train()\n    \n    epoch_loss = 0.0\n    \n    # Use tqdm for a nice progress bar\n    for src, tgt in tqdm(dataloader, desc=\"Training Epoch\"):\n        src = src.to(device)\n        tgt = tgt.to(device)\n        \n        optimizer.zero_grad()\n        \n        # We feed the decoder all tokens except the last one\n        # e.g., [<sos>, \"token1\", \"token2\"]\n        output = model(src, tgt[:, :-1])\n        \n        # We compare the output against all tokens except the first one\n        # e.g., [\"token1\", \"token2\", <eos>]\n        \n        # Reshape for CrossEntropyLoss\n        # Output: [Batch * SeqLen-1, VocabSize]\n        # Target: [Batch * SeqLen-1]\n        output_dim = output.shape[-1]\n        output_reshaped = output.contiguous().view(-1, output_dim)\n        tgt_reshaped = tgt[:, 1:].contiguous().view(-1)\n        \n        loss = criterion(output_reshaped, tgt_reshaped)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    # Return the average loss for this epoch\n    return epoch_loss / len(dataloader)\n\nprint(\" train_epoch function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:45:59.336947Z","iopub.execute_input":"2025-10-20T07:45:59.338178Z","iopub.status.idle":"2025-10-20T07:45:59.348810Z","shell.execute_reply.started":"2025-10-20T07:45:59.338143Z","shell.execute_reply":"2025-10-20T07:45:59.347806Z"}},"outputs":[{"name":"stdout","text":" train_epoch function defined.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Evaluation Loop Function (One Epoch)","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef evaluate_bleu(model, dataloader, tokenizer, device):\n    \"\"\"\n    Evaluates the model on the validation dataset and returns the BLEU score.\n    \"\"\"\n    model.eval() # Set model to evaluation mode\n    \n    all_references = [] # Ground truth\n    all_hypotheses = [] # Predictions\n    \n    # We don't need to track gradients\n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Calculating Val BLEU\"):\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            # --- Perform REAL Inference (Greedy Search) ---\n            # This is slow, but it's the CORRECT way to evaluate\n            \n            # 1. Run Encoder (once per batch)\n            src_mask = (src != PAD_ID).unsqueeze(1).unsqueeze(2)\n            enc_output = model.encode(src, src_mask)\n            \n            # 2. Run Decoder (token by token for each item in batch)\n            batch_size = src.shape[0]\n            \n            # Start decoder input with <sos> for all items in batch\n            tgt_tokens = torch.full((batch_size, 1), SOS_ID, dtype=torch.long, device=device)\n\n            for _ in range(MAX_LEN - 1): # Loop up to max length\n                tgt_len = tgt_tokens.shape[1]\n                \n                # Create decoder look-ahead mask\n                tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()\n                tgt_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(1) # [1, 1, len, len]\n                \n                # Decoder pass\n                dec_output = model.decode(tgt_tokens, enc_output, src_mask, tgt_mask)\n                \n                # Get logits for the *last* token\n                last_token_logits = model.fc_out(dec_output[:, -1, :])\n                \n                # Get the predicted token (Greedy)\n                pred_token = last_token_logits.argmax(dim=-1).unsqueeze(1) # [B, 1]\n                \n                # Append the new token to our running target\n                tgt_tokens = torch.cat((tgt_tokens, pred_token), dim=1)\n            \n            # --- End of Greedy Search ---\n            \n            # 'tgt_tokens' now contains the full predicted sentences\n            # 'tgt' contains the ground truth\n            \n            # Decode predictions\n            hyps = decode_batch(tgt_tokens, tokenizer) # decode_batch is from Cell 23\n            all_hypotheses.extend(hyps)\n            \n            # Decode references (ground truth)\n            refs = decode_batch(tgt[:, 1:], tokenizer) # Use tgt[:, 1:] (skip <sos>)\n            all_references.extend([[r] for r in refs])\n\n    # --- Calculate Final BLEU Score ---\n    bleu_score = corpus_bleu(all_references, all_hypotheses)\n    \n    return bleu_score\n\n# --- Test the new evaluate function ---\n# (Assuming 'model' is loaded from Cell 18)\n# (Assuming 'decode_batch' is defined in Cell 23... let's define it here just in case)\n\ndef decode_batch(batch_tensor, tokenizer):\n    \"\"\"Helper function to decode tensors (if not defined in Cell 23 yet).\"\"\"\n    text_list = []\n    for tensor in batch_tensor:\n        ids = tensor.cpu().numpy()\n        ids = [int(id) for id in ids if id not in (PAD_ID, SOS_ID, EOS_ID)]\n        text = tokenizer.decode(ids)\n        text_list.append(text)\n    return text_list\n\nprint(\" 'evaluate_bleu' function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:46:20.907528Z","iopub.execute_input":"2025-10-20T07:46:20.908261Z","iopub.status.idle":"2025-10-20T07:46:20.919396Z","shell.execute_reply.started":"2025-10-20T07:46:20.908235Z","shell.execute_reply":"2025-10-20T07:46:20.918604Z"}},"outputs":[{"name":"stdout","text":" 'evaluate_bleu' function defined.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### 2) Run the Main Training Loop","metadata":{}},{"cell_type":"markdown","source":"## 1) Load Best Model and Set to Eval Mode","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 30 \nbest_val_bleu = 0.0 \nMODEL_SAVE_PATH = 'best_model.pth'\n\nprint(f\"Starting training for {N_EPOCHS} epochs on {DEVICE}...\")\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    # --- Training (calculates loss) ---\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE, clip=1)\n    \n    # --- Evaluation (calculates BLEU) ---\n    # We use the new function\n    val_bleu = evaluate_bleu(model, val_loader, sp, DEVICE)\n    \n    end_time = time.time()\n    \n    epoch_mins = int((end_time - start_time) / 60)\n    epoch_secs = int((end_time - start_time) % 60)\n    \n    # Calculate perplexity (PPL) just for printing\n    train_ppl = math.exp(train_loss)\n    \n    print(f\"\\n--- Epoch {epoch+1:02} / {N_EPOCHS} | Time: {epoch_mins}m {epoch_secs}s ---\")\n    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}\")\n    # We now print Validation BLEU\n    print(f\"\\tVal. BLEU:  {val_bleu:.4f}\")\n    \n    # --- Save the best model (based on BLEU) ---\n    if val_bleu > best_val_bleu:\n        best_val_bleu = val_bleu\n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        print(f\"\\t✨ New best model saved! (Val. BLEU: {best_val_bleu:.4f})\")\n\nprint(f\"\\n Training complete. Best model (BLEU: {best_val_bleu:.4f}) saved to {MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:46:30.500543Z","iopub.execute_input":"2025-10-20T07:46:30.501220Z","iopub.status.idle":"2025-10-20T07:53:50.660339Z","shell.execute_reply.started":"2025-10-20T07:46:30.501194Z","shell.execute_reply":"2025-10-20T07:53:50.659696Z"}},"outputs":[{"name":"stdout","text":"Starting training for 30 epochs on cuda...\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 33.10it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 01 / 30 | Time: 0m 15s ---\n\tTrain Loss: 6.285 | Train PPL: 536.486\n\tVal. BLEU:  0.0629\n\t✨ New best model saved! (Val. BLEU: 0.0629)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.84it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 02 / 30 | Time: 0m 14s ---\n\tTrain Loss: 5.634 | Train PPL: 279.800\n\tVal. BLEU:  0.0864\n\t✨ New best model saved! (Val. BLEU: 0.0864)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 33.71it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 03 / 30 | Time: 0m 15s ---\n\tTrain Loss: 5.438 | Train PPL: 229.947\n\tVal. BLEU:  0.0986\n\t✨ New best model saved! (Val. BLEU: 0.0986)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 33.67it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 04 / 30 | Time: 0m 14s ---\n\tTrain Loss: 5.266 | Train PPL: 193.734\n\tVal. BLEU:  0.0986\n\t✨ New best model saved! (Val. BLEU: 0.0986)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.68it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 05 / 30 | Time: 0m 14s ---\n\tTrain Loss: 5.103 | Train PPL: 164.460\n\tVal. BLEU:  0.0932\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 35.12it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 06 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.948 | Train PPL: 140.867\n\tVal. BLEU:  0.1057\n\t✨ New best model saved! (Val. BLEU: 0.1057)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.87it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 07 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.803 | Train PPL: 121.844\n\tVal. BLEU:  0.1123\n\t✨ New best model saved! (Val. BLEU: 0.1123)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.56it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 08 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.664 | Train PPL: 106.065\n\tVal. BLEU:  0.1065\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.46it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 09 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.531 | Train PPL:  92.894\n\tVal. BLEU:  0.1076\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.52it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 10 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.399 | Train PPL:  81.394\n\tVal. BLEU:  0.1079\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.68it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 11 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.266 | Train PPL:  71.264\n\tVal. BLEU:  0.1189\n\t✨ New best model saved! (Val. BLEU: 0.1189)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.71it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 12 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.142 | Train PPL:  62.955\n\tVal. BLEU:  0.1027\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.78it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 13 / 30 | Time: 0m 14s ---\n\tTrain Loss: 4.026 | Train PPL:  56.059\n\tVal. BLEU:  0.1103\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.79it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 14 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.904 | Train PPL:  49.579\n\tVal. BLEU:  0.1238\n\t✨ New best model saved! (Val. BLEU: 0.1238)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.71it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 15 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.782 | Train PPL:  43.909\n\tVal. BLEU:  0.1171\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.71it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 16 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.662 | Train PPL:  38.932\n\tVal. BLEU:  0.1149\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.58it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 17 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.549 | Train PPL:  34.792\n\tVal. BLEU:  0.1163\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.69it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 18 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.433 | Train PPL:  30.969\n\tVal. BLEU:  0.1138\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.55it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 19 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.318 | Train PPL:  27.610\n\tVal. BLEU:  0.1165\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.62it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 20 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.207 | Train PPL:  24.697\n\tVal. BLEU:  0.1169\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.44it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 21 / 30 | Time: 0m 14s ---\n\tTrain Loss: 3.097 | Train PPL:  22.121\n\tVal. BLEU:  0.1158\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.21it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 22 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.997 | Train PPL:  20.022\n\tVal. BLEU:  0.1105\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.59it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 23 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.877 | Train PPL:  17.765\n\tVal. BLEU:  0.1186\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.53it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 24 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.776 | Train PPL:  16.049\n\tVal. BLEU:  0.1093\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.59it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 25 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.671 | Train PPL:  14.451\n\tVal. BLEU:  0.1196\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.69it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 26 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.563 | Train PPL:  12.976\n\tVal. BLEU:  0.1207\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.66it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 27 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.461 | Train PPL:  11.713\n\tVal. BLEU:  0.1247\n\t✨ New best model saved! (Val. BLEU: 0.1247)\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.61it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 28 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.360 | Train PPL:  10.590\n\tVal. BLEU:  0.1172\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.67it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 29 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.256 | Train PPL:   9.545\n\tVal. BLEU:  0.1232\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch: 100%|██████████| 281/281 [00:08<00:00, 34.67it/s]\nCalculating Val BLEU: 100%|██████████| 36/36 [00:06<00:00,  5.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 30 / 30 | Time: 0m 14s ---\n\tTrain Loss: 2.166 | Train PPL:   8.721\n\tVal. BLEU:  0.1205\n\n Training complete. Best model (BLEU: 0.1247) saved to best_model.pth\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"model = Transformer(\n    src_vocab_size=VOCAB_SIZE,\n    tgt_vocab_size=VOCAB_SIZE,\n    d_model=EMBED_DIM,\n    num_heads=NUM_HEADS,\n    num_encoder_layers=ENCODER_LAYERS,\n    num_decoder_layers=DECODER_LAYERS,\n    d_ff=FFN_DIM,\n    max_len=MAX_LEN,\n    dropout=DROPOUT,\n    pad_idx=PAD_ID\n).to(DEVICE)\n\nMODEL_SAVE_PATH = 'best_model.pth'\nmodel.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\nmodel.eval()\n\nprint(f\" Best model loaded from '{MODEL_SAVE_PATH}' and set to eval mode.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:54:29.332442Z","iopub.execute_input":"2025-10-20T07:54:29.333153Z","iopub.status.idle":"2025-10-20T07:54:29.464461Z","shell.execute_reply.started":"2025-10-20T07:54:29.333127Z","shell.execute_reply":"2025-10-20T07:54:29.463814Z"}},"outputs":[{"name":"stdout","text":" Best model loaded from 'best_model.pth' and set to eval mode.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### 3) Calculate Final Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.chrf_score import corpus_chrf\nfrom rouge_score import rouge_scorer\nimport numpy as np\n\ndef decode_batch(batch_tensor, tokenizer):\n    text_list = []\n    for tensor in batch_tensor:\n        ids = tensor.cpu().numpy()\n        ids = [int(id) for id in ids if id not in (PAD_ID, SOS_ID, EOS_ID)]\n        text = tokenizer.decode(ids)\n        text_list.append(text)\n    return text_list\n\ndef calculate_metrics_real(model, dataloader, criterion, tokenizer, device):\n    \n    model.eval()\n    total_loss = 0.0\n    all_references = []\n    all_hypotheses = []\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(dataloader, desc=\"Calculating Final Metrics\"):\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            output_loss = model(src, tgt[:, :-1])\n            output_dim = output_loss.shape[-1]\n            output_reshaped = output_loss.contiguous().view(-1, output_dim)\n            tgt_reshaped = tgt[:, 1:].contiguous().view(-1)\n            loss = criterion(output_reshaped, tgt_reshaped)\n            total_loss += loss.item()\n            \n            # (This is the same logic from our new Cell 20)\n            src_mask = (src != PAD_ID).unsqueeze(1).unsqueeze(2)\n            enc_output = model.encode(src, src_mask)\n            batch_size = src.shape[0]\n            tgt_tokens = torch.full((batch_size, 1), SOS_ID, dtype=torch.long, device=device)\n\n            for _ in range(MAX_LEN - 1):\n                tgt_len = tgt_tokens.shape[1]\n                tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()\n                tgt_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(1)\n                dec_output = model.decode(tgt_tokens, enc_output, src_mask, tgt_mask)\n                last_token_logits = model.fc_out(dec_output[:, -1, :])\n                pred_token = last_token_logits.argmax(dim=-1).unsqueeze(1)\n                tgt_tokens = torch.cat((tgt_tokens, pred_token), dim=1)\n            \n            # Decode results\n            hyps = decode_batch(tgt_tokens, tokenizer)\n            refs = decode_batch(tgt[:, 1:], tokenizer)\n            \n            # ROUGE-L Fix: Only add non-empty pairs\n            for r, h in zip(refs, hyps):\n                if r.strip() and h.strip():\n                    all_references.append([r])\n                    all_hypotheses.append(h)\n\n    avg_loss = total_loss / len(dataloader)\n    perplexity = math.exp(avg_loss)\n    \n    if not all_hypotheses or not all_references:\n        print(\"Warning: No valid metrics found.\")\n        return {'Perplexity': perplexity, 'BLEU': 0.0, 'ROUGE-L': 0.0, 'chrF': 0.0}\n\n    bleu_score = corpus_bleu(all_references, all_hypotheses)\n    rouge_l_scores = []\n    for ref_list, hyp in zip(all_references, all_hypotheses):\n        score = scorer.score(ref_list[0], hyp)\n        rouge_l_scores.append(score['rougeL'].fmeasure)\n    rouge_l_avg = np.mean(rouge_l_scores) if rouge_l_scores else 0.0\n    chrf_score = corpus_chrf([ref[0] for ref in all_references], all_hypotheses)\n    \n    return {\n        'Perplexity': perplexity,\n        'BLEU': bleu_score,\n        'ROUGE-L': rouge_l_avg,\n        'chrF': chrf_score\n    }\n\nprint(\"Calculating evaluation metrics on the test set...\")\n# (Assuming 'model' is loaded from Cell 22)\nmetrics = calculate_metrics_real(model, test_loader, criterion, sp, DEVICE)\n\nprint(\"\\n--- Evaluation Results (Test Set) ---\")\nprint(f\"  Perplexity: {metrics['Perplexity']:.3f}\")\nprint(f\"  BLEU:       {metrics['BLEU']:.4f}\")\nprint(f\"  ROUGE-L:    {metrics['ROUGE-L']:.4f}\")\nprint(f\"  chrF:       {metrics['chrF']:.4f}\")\nprint(\"---------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:54:32.681685Z","iopub.execute_input":"2025-10-20T07:54:32.681952Z","iopub.status.idle":"2025-10-20T07:54:39.551012Z","shell.execute_reply.started":"2025-10-20T07:54:32.681933Z","shell.execute_reply":"2025-10-20T07:54:39.550417Z"}},"outputs":[{"name":"stdout","text":"Calculating evaluation metrics on the test set...\n","output_type":"stream"},{"name":"stderr","text":"Calculating Final Metrics: 100%|██████████| 36/36 [00:06<00:00,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluation Results (Test Set) ---\n  Perplexity: 274.712\n  BLEU:       0.1192\n  ROUGE-L:    0.0000\n  chrF:       0.1057\n---------------------------------------\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Inference Function (Greedy Search)","metadata":{}},{"cell_type":"code","source":"def predict_sentence(sentence, model, sp_tokenizer, device, max_len=MAX_LEN):\n\n    \n    model.eval()\n    \n    normalized_sentence = normalize_urdu_text(sentence)\n    token_ids = sp_tokenizer.encode(normalized_sentence, out_type=int)\n    \n    src_tokens = [SOS_ID] + token_ids[:max_len - 2] + [EOS_ID]\n    \n    pad_len = max_len - len(src_tokens)\n    src_tokens += [PAD_ID] * pad_len\n    \n    src_tensor = torch.tensor([src_tokens], dtype=torch.long).to(device)\n    \n    # --- Encoder Pass ---\n    with torch.no_grad():\n        src_mask = (src_tensor != PAD_ID).unsqueeze(1).unsqueeze(2)\n        \n        # enc_output shape: [1, src_seq_len, d_model]\n        enc_output = model.encode(src_tensor, src_mask)\n        \n    # --- Decoder Loop (Greedy Search) ---\n    \n    # tgt_tokens shape: [1, 1]\n    tgt_tokens = torch.tensor([[SOS_ID]], dtype=torch.long).to(device)\n    \n    # Loop for a maximum of max_len steps\n    for i in range(max_len):\n        with torch.no_grad():\n            tgt_len = tgt_tokens.shape[1]\n            # [tgt_len, tgt_len]\n            tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()\n            # We don't need a padding mask for the target during inference\n            tgt_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(1) # [1, 1, tgt_len, tgt_len]\n\n            # dec_output shape: [1, tgt_len, d_model]\n            dec_output = model.decode(tgt_tokens, enc_output, src_mask, tgt_mask)\n            \n            # 11. Get logits for the *very ast* token\n            # last_token_logits shape: [1, d_model]\n            last_token_logits = model.fc_out(dec_output[:, -1, :])\n            \n            # pred_token shape: [1]\n            pred_token = last_token_logits.argmax(dim=-1)\n            \n            # This will be used as input in the next loop iteration\n            # tgt_tokens shape: [1, tgt_len + 1]\n            tgt_tokens = torch.cat((tgt_tokens, pred_token.unsqueeze(0)), dim=1)\n            \n            if pred_token.item() == EOS_ID:\n                break\n\n    output_ids = tgt_tokens.squeeze(0).cpu().numpy()[1:] # Remove <sos>\n    # Filter out <eos> before decoding\n    output_text = sp_tokenizer.decode([int(id) for id in output_ids if id != EOS_ID])\n    \n    return output_text\n\n# --- Test the inference function ---\nprint(\"Testing inference function...\")\n# We use a sentence from our validation set for a fair test\ntest_input_sentence = val_df.iloc[10]['input']\npredicted_response = predict_sentence(test_input_sentence, model, sp, DEVICE)\n\nprint(f\"Input:    '{test_input_sentence}'\")\nprint(f\"Response: '{predicted_response}'\")\nprint(f\"Expected: '{val_df.iloc[10]['response']}'\")\nprint(\"\\n Inference function created and tested.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:54:57.089325Z","iopub.execute_input":"2025-10-20T07:54:57.089647Z","iopub.status.idle":"2025-10-20T07:54:57.117325Z","shell.execute_reply.started":"2025-10-20T07:54:57.089628Z","shell.execute_reply":"2025-10-20T07:54:57.116709Z"}},"outputs":[{"name":"stdout","text":"Testing inference function...\nInput:    'کہ نبی صلی اللہ'\nResponse: 'وسلم مسجد میں بھی ہیں۔'\nExpected: 'علیہ وسلم نے فرمایا'\n\n Inference function created and tested.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### Gradio UI Interface","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport time\n\nprint(\"Launching Improved Gradio Interface...\")\n\ndef chatbot_response(input_text, history):\n    \"\"\"\n    Main function called by Gradio.\n    'history' is a list of [user_msg, bot_msg] pairs.\n    \"\"\"\n    if not input_text:\n        history.append((None, \"برائے مہربانی، کچھ لکھ کر بھیجیں۔\"))\n        return \"\", history # Return empty string, updated history\n    \n    try:\n        # Use the inference function\n        response = predict_sentence(input_text, model, sp, DEVICE)\n        \n        # Add the new [user_msg, bot_msg] pair to the history\n        history.append((input_text, response))\n        \n        # Return empty string to clear the textbox, and the updated history\n        return \"\", history\n    \n    except Exception as e:\n        history.append((input_text, f\"ایک خرابی پیش آ گئی: {str(e)}\"))\n        return \"\", history\n\ndef clear_chat():\n    \"\"\"Clears the chat history.\"\"\"\n    return None \n\ncss = \"\"\"\n.gradio-container { direction: rtl; }\ntextarea[data-testid=\"textbox\"] { text-align: right; }\n\"\"\"\n\nwith gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:\n    gr.Markdown(\"# 🤖 اردو ٹرانسفارmer چیٹ بوٹ (From Scratch)\")\n    gr.Markdown(\"یہ چیٹ بوٹ PyTorch Transformer (Encoder-Decoder) کا استعمال کرتے ہوئے بنایا گیا ہے۔\")\n    \n    with gr.Row():\n        # Chatbot component for history\n        chatbot = gr.Chatbot(label=\"گفتگو\", height=400, rtl=True, show_label=False)\n        \n    with gr.Row():\n        # Textbox for user input\n        txt_input = gr.Textbox(\n            label=\"آپ کا پیغام\",\n            placeholder=\"آپ کا پیغام یہاں لکھیں...\",\n            lines=2,\n            rtl=True,\n            scale=4 # Make textbox bigger\n        )\n        \n        # Submit button\n        btn_submit = gr.Button(\"بھیجیں\", variant=\"primary\", scale=1) # \"Send\" button\n        \n    with gr.Row():\n        # 'Clear' button\n        btn_clear = gr.Button(\"گفتگو صاف کریں\", variant=\"secondary\") # \"Clear Chat\" button\n        \n    \n    # Function to handle submission\n    def submit_message(input_text, history):\n        return chatbot_response(input_text, history)\n\n    txt_input.submit(\n        fn=submit_message,\n        inputs=[txt_input, chatbot],\n        outputs=[txt_input, chatbot]\n    )\n    \n    btn_submit.click(\n        fn=submit_message,\n        inputs=[txt_input, chatbot],\n        outputs=[txt_input, chatbot]\n    )\n    \n    btn_clear.click(\n        fn=clear_chat,\n        inputs=None,\n        outputs=[chatbot]\n    )\n    \ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:56.320705Z","iopub.execute_input":"2025-10-20T07:56:56.320987Z","iopub.status.idle":"2025-10-20T08:02:18.020851Z","shell.execute_reply.started":"2025-10-20T07:56:56.320968Z","shell.execute_reply":"2025-10-20T08:02:18.020211Z"}},"outputs":[{"name":"stdout","text":"Launching Improved Gradio Interface...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3499474058.py:44: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(label=\"گفتگو\", height=400, rtl=True, show_label=False)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://29a55c0201745833d7.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://29a55c0201745833d7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://29a55c0201745833d7.gradio.live\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"### Qualitative Testing","metadata":{}},{"cell_type":"code","source":"\nprint(\"--- Final Qualitative Testing ---\")\n\nmodel.eval()\n\ntest_sentences_good = [\n    \"کیا ہم یہ\",         # Aap ka test case\n    \"یہ ایک\",             # Bohat aam jumla\n    \"پاکستان کا سب سے بڑا\", # Dekhte hain kya kehta hai\n    \"وہ سکول\",\n    \"رات کے وقت\",\n    \"اس نے مجھے\"\n]\n\ntest_sentences_bad = [\n    \"السلام علیکم\",\n    \"آپ کا نام کیا ہے؟\",\n    \"لاہور\"\n]\n\nprint(\"\\n--- Testing: Sentence Completion (Good Tests) ---\")\nfor sentence in test_sentences_good:\n    response = predict_sentence(sentence, model, sp, DEVICE)\n    print(f\"Input:    '{sentence}'\")\n    print(f\"Response: '{response}'\")\n    print(\"-----\")\n\nprint(\"\\n--- Testing: Conversational (Bad Tests) ---\")\nfor sentence in test_sentences_bad:\n    response = predict_sentence(sentence, model, sp, DEVICE)\n    print(f\"Input:    '{sentence}'\")\n    print(f\"Response: '{response}'\")\n    print(\"-----\")\n\nprint(\" Qualitative testing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:02:18.545121Z","iopub.execute_input":"2025-10-20T08:02:18.545311Z","iopub.status.idle":"2025-10-20T08:02:18.675695Z","shell.execute_reply.started":"2025-10-20T08:02:18.545296Z","shell.execute_reply":"2025-10-20T08:02:18.675134Z"}},"outputs":[{"name":"stdout","text":"--- Final Qualitative Testing ---\n\n--- Testing: Sentence Completion (Good Tests) ---\nInput:    'کیا ہم یہ'\nResponse: 'کر سکتے ہیں؟'\n-----\nInput:    'یہ ایک'\nResponse: 'بارڈر ہے۔'\n-----\nInput:    'پاکستان کا سب سے بڑا'\nResponse: 'بھارت میں بہت کم ہے'\n-----\nInput:    'وہ سکول'\nResponse: 'پیدا ہوتے ہیں۔'\n-----\nInput:    'رات کے وقت'\nResponse: 'کارواں نہیں۔'\n-----\nInput:    'اس نے مجھے'\nResponse: 'کیوں نکالا ہے'\n-----\n\n--- Testing: Conversational (Bad Tests) ---\nInput:    'السلام علیکم'\nResponse: 'بھی کہا جاتا'\n-----\nInput:    'آپ کا نام کیا ہے؟'\nResponse: 'اس کے لیے نہیں کیا جايے؟'\n-----\nInput:    'لاہور'\nResponse: 'قلندرز کے'\n-----\n Qualitative testing complete.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}